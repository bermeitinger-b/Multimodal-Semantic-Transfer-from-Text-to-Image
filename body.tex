% !TeX spellcheck = en_US
% !TeX encoding = UTF-8
\section{Introduction}
In the last years, image classification processes like neural networks in the area of art-history and \emph{Heritage Informatics} have experienced a broad distribution \parencite{lang_AttestingSimilaritySupportingOrganizationStudy_2018}.
These methods face a number of challenges, including the handling of comparatively small amounts of data as well as high-dimensional data in the Digital Humanities. In most cases, these methods map the classification task to a flat target space. This \enquote{flat} surface loses a number of relevant dimensions in the search for ontological uniqueness, including taxonomical, mereological, and associative relationships between classes, or the non-formal context, respectively.

The proposed solution by \citeauthor{donig_VomBildTextUndWieder_2019a} \parencite{donig_VomBildTextUndWieder_2019a} to expand the capabilities of visual classifiers is to take advantage of the greater expressiveness of text-based models. Here, a \emph{Convolutional Neural Network} (CNN) is used that output is not as usual a series of flat text labels but a series of semantically loaded vectors. These vectors result from a \emph{Distributional Semantic Model} (DSM) (\cite{lenci_DistributionalModelsWordMeaning_2018a}) which is generated from an in-domain text corpus.


Here, we propose an early implementation of this method and analyze the results.

The conducted experiment is based on the collation of two corpora: one text-based and a visual. From the text, a DSM is created and then queried for a list of target words that are functionally the labels that are manually given to the images. The result is a list of vectors that correspond to the target words leading to images that are annotated not only with a label but also with a unique vector. The images and vectors are used as training data for a CNN that, afterwards, should be able to predict a vector for an unseen image. This prediction vector can be converted back to a word by the DSM using a nearest-neighbors algorithm. We are looking for richer representation in this process, so we choose the five nearest neighbors. The similarity measure is the cosine similarity for high-dimensional vector spaces between the given target vector and the prediction vector. We derive a positive classification result if the target label is within the list of five nearest neighbors of the prediction vector.

Moreover, we compare the results between the proposed classification method and a conventional classificaiton method using the same CNN as for the vector-based experiment but a list of flat labels. Finally, we can show that the vector-based approach (judging from classification metrics) is equally performant or even better that the label-based version.


\section{Experiment Structure}
The experiment is based on one text and one visual corpus from the area of material culture research with a focus on neo-classical artifacts.

\subsection{Text Corpus}
The text corpus is compiled out of 44 sources that are available under a free and permissive license. It contains English specialist publications on furniture and spatial art, published from the end of the 19\textsuperscript{th} century to the middle of the 20\textsuperscript{th} century. In multiple steps, the corpus is cleaned an preprocessed. First, a series of standard NLP methods are applied like tokenization, sentence- and word splitting, normaliziation of numbers, and named entitity recognition (NER). Since we used retro-digitized material from different source, we implemented manual corrections for the most common errors (such as ligatures like \texttt{TT} that were misinterpreted as \texttt{U}). Another level of preprocessing consists of content-related augmentations. In particular, we normalized compound words and synonyms according to a specified list, which is based on an ontology, the \emph{Neoclassica-Ontology} \parencite{donig_NeoclassicaMultilingualDomainOntology_2016}. This resulted in the final text corpus of total \num{3067237} words comprised of \num{107518} basic word forms.

The DSM is created using the \emph{Indra Frameworks} \parencite{sales_IndraWordEmbeddingSemanticRelatedness_2018a} 